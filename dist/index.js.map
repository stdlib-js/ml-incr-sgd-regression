{
  "version": 3,
  "sources": ["../lib/dot.js", "../lib/weight_vector.js", "../lib/regularize.js", "../lib/loss/epsilon_insensitive.js", "../lib/loss/squared_error.js", "../lib/loss/huber.js", "../lib/eta_factory.js", "../lib/defaults.json", "../lib/validate.js", "../lib/main.js", "../lib/index.js"],
  "sourcesContent": ["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Calculates the dot product of two vectors.\n*\n* @private\n* @param {NumericArray} x - first vector\n* @param {NumericArray} y - second vector\n* @returns {number} dot product\n*\n* @example\n* var x = [ 1.0, 2.0, 3.0 ];\n* var y = [ 1.0, 2.0, 2.0 ];\n*\n* var ret = dot( x, y );\n* // returns 11.0\n*/\nfunction dot( x, y ) {\n\tvar len = x.length;\n\tvar ret = 0;\n\tvar i;\n\n\tfor ( i = 0; i < len; i++ ) {\n\t\tret += x[ i ] * y[ i ];\n\t}\n\treturn ret;\n}\n\n\n// EXPORTS //\n\nmodule.exports = dot;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* The weight vector implementation was inspired by the [sofia-ml]{@link https://code.google.com/archive/p/sofia-ml/} library.\n*/\n\n// MODULES //\n\nvar isPositiveInteger = require( '@stdlib/assert-is-positive-integer' );\nvar isBoolean = require( '@stdlib/assert-is-boolean' );\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar format = require( '@stdlib/string-format' );\nvar pow = require( '@stdlib/math-base-special-pow' );\nvar dot = require( './dot.js' );\n\n\n// VARIABLES //\n\nvar MIN_SCALE = 1.0e-11;\n\n\n// FUNCTIONS //\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @private\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nfunction scaleTo( factor ) {\n\t/* eslint-disable no-invalid-this */\n\tvar i;\n\tif ( this.scale < MIN_SCALE ) {\n\t\t// Scale vector to one:\n\t\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\t\tthis._data[ i ] *= this.scale;\n\t\t}\n\t\tthis.scale = 1.0;\n\t}\n\n\tthis.norm *= pow( factor, 2 );\n\n\tif ( factor > 0.0 ) {\n\t\tthis.scale *= factor;\n\t} else {\n\t\tthrow new RangeError( format( 'unexpected error. Scaling weight vector by nonpositive value, likely due to too large value of eta * lambda. Value: `%f`.', factor ) );\n\t}\n}\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @private\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nfunction add( x, xScale ) {\n\t/* eslint-disable no-invalid-this */\n\tvar xscaled;\n\tvar inner;\n\tvar i;\n\n\tinner = 0.0;\n\tif ( xScale === void 0 ) {\n\t\txScale = 1.0;\n\t}\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\txscaled = x[ i ] * xScale;\n\t\tinner += this._data[i] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this.intercept ) {\n\t\txscaled = 1.0 * xScale;\n\t\tinner += this._data[ i ] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\tthis.norm += ( ( dot( x, x ) + ( ( this.intercept ) ? 1.0 : 0.0 ) ) *\n\t\tpow( xScale, 2 ) ) +\n\t\t( 2.0 * this.scale * inner );\n}\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @private\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nfunction innerProduct( x ) {\n\t/* eslint-disable no-invalid-this */\n\tvar ret = 0;\n\tvar i;\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\tret += this._data[ i ] * x[ i ];\n\t}\n\tret += ( this.intercept ) ? this._data[ i ] : 0.0;\n\tret *= this.scale;\n\treturn ret;\n}\n\n\n// MAIN //\n\n/**\n* Creates a WeightVector.\n*\n* @constructor\n* @param {PositiveInteger} dim - number of feature weights (excluding bias/intercept term)\n* @param {boolean} intercept - boolean indicating whether a bias/intercept weight should be implicitly assumed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a boolean\n*/\nfunction WeightVector( dim, intercept ) {\n\tvar i;\n\tif ( !(this instanceof WeightVector) ) {\n\t\treturn new WeightVector( dim, intercept );\n\t}\n\tif ( !isPositiveInteger( dim ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. First argument must be a positive integer. Value: `%s`.', dim ) );\n\t}\n\tif ( !isBoolean( intercept ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. Second argument must be a boolean. Value: `%s`.', intercept ) );\n\t}\n\n\tthis.scale = 1.0;\n\tthis.norm = 0.0;\n\tthis.intercept = intercept;\n\tthis.nWeights = dim + ( ( this.intercept ) ? 1 : 0 );\n\n\tthis._data = new Array( this.nWeights );\n\n\t// Initialize weights to zero:\n\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\tthis._data[ i ] = 0.0;\n\t}\n}\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @memberof WeightVector.prototype\n* @function scaleTo\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nsetReadOnly( WeightVector.prototype, 'scaleTo', scaleTo );\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @memberof WeightVector.prototype\n* @function add\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nsetReadOnly( WeightVector.prototype, 'add', add );\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @memberof WeightVector.prototype\n* @function innerProduct\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nsetReadOnly( WeightVector.prototype, 'innerProduct', innerProduct );\n\n\n// EXPORTS //\n\nmodule.exports = WeightVector;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar max = require( '@stdlib/math-base-special-max' );\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1e-7;\n\n\n// MAIN //\n\n/**\n* L2 regularization of feature weights.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} eta - current learning rate\n*/\nfunction regularize( weights, lambda, eta ) {\n\tvar scalingFactor;\n\tif ( lambda > 0.0 ) {\n\t\tscalingFactor = 1.0 - ( eta * lambda );\n\t\tweights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = regularize;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar regularize = require( './../regularize.js' );\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the epsilon-insensitive loss.\n*\n* ## Notes\n*\n* The penalty of the epsilon-insensitive loss is the absolute value of the dot product of the weights and `x` minus `y` whenever the absolute error exceeds epsilon, and zero otherwise.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction epsilonInsensitiveLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = epsilonInsensitiveLoss;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar regularize = require( './../regularize.js' );\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the squared error loss.\n*\n* ## Notes\n*\n* The squared error loss is defined as the squared difference of the observed and fitted value.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction squaredErrorLoss( weights, x, y, eta, lambda ) {\n\tvar loss = y - weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tweights.add( x, ( eta * loss ) );\n}\n\n\n// EXPORTS //\n\nmodule.exports = squaredErrorLoss;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar regularize = require( './../regularize.js' );\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the [Huber loss][1] function.\n*\n* ## Notes\n*\n* The Huber loss uses squared-error loss for observations with error smaller than epsilon in magnitude and linear loss above that in order to decrease the influence of outliers on the model fit.\n*\n* [1]: https://en.wikipedia.org/wiki/Huber_loss\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction huberLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t} else {\n\t\tweights.add( x, -eta * p );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = huberLoss;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar format = require( '@stdlib/string-format' );\n\n\n// MAIN //\n\n/**\n* Returns a function to retrieve the current learning rate.\n*\n* @private\n* @param {string} type - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {PositiveNumber} eta0 - constant learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @throws {Error} first argument must be `basic`, `constant` or `pegasos`\n* @returns {Function} getEta function\n*/\nfunction closure( type, eta0, lambda ) {\n\tvar iter;\n\tvar ret;\n\n\titer = 1;\n\n\tswitch ( type ) {\n\tcase 'basic':\n\t\t// Default case: 'basic'\n\t\tret = getEtaBasic;\n\t\tbreak;\n\tcase 'constant':\n\t\tret = getEtaConstant;\n\t\tbreak;\n\tcase 'pegasos':\n\t\tret = getEtaPegasos;\n\t\tbreak;\n\tdefault:\n\t\tthrow new Error( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'learningRate', [ 'basic', 'constant', 'pegasos' ].join( '\", \"' ), type ) );\n\t}\n\treturn ret;\n\n\t/**\n\t* Returns the basic learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaBasic() {\n\t\tvar eta = 1000.0 / ( iter + 1000.0 );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n\n\t/**\n\t* Returns the constant learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaConstant() {\n\t\titer += 1;\n\t\treturn eta0;\n\t}\n\n\t/**\n\t* Returns the Pegasos learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaPegasos() {\n\t\tvar eta = 1.0 / ( lambda * iter );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = closure;\n", "{\n    \"epsilon\": 0.1,\n    \"eta0\": 0.02,\n    \"intercept\": true,\n    \"lambda\": 1e-3,\n    \"learningRate\": \"basic\",\n    \"loss\": \"squaredError\"\n}\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar isNonNegativeNumber = require( '@stdlib/assert-is-nonnegative-number' ).isPrimitive;\nvar isPositiveNumber = require( '@stdlib/assert-is-positive-number' ).isPrimitive;\nvar isBoolean = require( '@stdlib/assert-is-boolean' ).isPrimitive;\nvar isObject = require( '@stdlib/assert-is-plain-object' );\nvar isString = require( '@stdlib/assert-is-string' ).isPrimitive;\nvar hasOwnProp = require( '@stdlib/assert-has-own-property' );\nvar format = require( '@stdlib/string-format' );\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.epsilon] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0] - constant learning rate\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {string} [options.learningRate] - the learning rate to use\n* @param {string} [options.loss] -  the loss function to use\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( 'invalid argument. Options argument must be an object. Value: `%s`.', options ) );\n\t}\n\tif ( hasOwnProp( options, 'epsilon' ) ) {\n\t\topts.epsilon = options.epsilon;\n\t\tif ( !isPositiveNumber( opts.epsilon ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a positive number. Option: `%s`.', 'epsilon', opts.epsilon ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'eta0' ) ) {\n\t\topts.eta0 = options.eta0;\n\t\tif ( !isPositiveNumber( opts.eta0 ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a positive number. Option: `%s`.', 'eta0', opts.eta0 ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a nonnegative number. Option: `%s`.', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\topts.learningRate = options.learningRate;\n\t\tif ( !isString( opts.learningRate ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a string. Option: `%s`.', 'learningRate', opts.learningRate ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isString( opts.loss ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a string. Option: `%s`.', 'loss', opts.loss ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nmodule.exports = validate;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable */ // TODO: fix me\n'use strict';\n\n// MODULES //\n\nvar isArray = require( '@stdlib/assert-is-array' );\nvar format = require( '@stdlib/string-format' );\nvar copy = require( '@stdlib/utils-copy' );\nvar setNonEnumerableReadOnlyAccessor = require( '@stdlib/utils-define-nonenumerable-read-only-accessor' );\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar WeightVector = require( './weight_vector.js' );\nvar epsilonInsensitiveLoss = require( './loss/epsilon_insensitive.js' );\nvar squaredErrorLoss = require( './loss/squared_error.js' );\nvar huberLoss = require( './loss/huber.js' );\nvar getEta = require( './eta_factory.js' );\nvar DEFAULTS = require( './defaults.json' );\nvar validate = require( './validate.js' );\n\n\n// MAIN //\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* The sub-gradient of the loss function is estimated for each datum and the regression model is updated incrementally, with a decreasing learning rate and regularization of the feature weights based on L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3\u201330. doi:10.1007/s10107-010-0420-4\n*\n* @param {Object} [options] - options object\n* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate\n* @param {NonNegativeNumber} [options.lambda=1e-3] - regularization parameter\n* @param {string} [options.learningRate='basic'] - string denoting the learning rate to use. Can be `constant`, `pegasos`, or `basic`\n* @param {string} [options.loss='squaredError'] - string denoting the loss function to use. Can be `squaredError`, `epsilonInsensitive`, or `huber`\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Object} regression model\n*\n* @example\n* var incrSGDRegression = require( '@stdlib/streams-ml-incr-sgd-regression' );\n*\n* var accumulator = incrSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* // Update model as observations come in:\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* accumulator( x, y );\n*\n* // Predict new observation:\n* var yHat = accumulator.predict( x );\n*\n* // Retrieve coefficients:\n* var coefs = accumulator.coefs;\n*/\nfunction incrSGDRegression( options ) {\n\tvar _nFeatures;\n\tvar _lossfun;\n\tvar _weights;\n\tvar _getEta;\n\tvar accumulator;\n\tvar opts;\n\tvar err;\n\n\topts = copy( DEFAULTS );\n\tif ( arguments.length > 0 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\t_weights = null;\n\n\t// Set loss function:\n\tswitch ( opts.loss ) {\n\tcase 'epsilonInsensitive':\n\t\t_lossfun = epsilonInsensitiveLoss;\n\tbreak;\n\tcase 'huber':\n\t\t_lossfun = huberLoss;\n\tbreak;\n\tcase 'squaredError':\n\t\t_lossfun = squaredErrorLoss;\n\tbreak;\n\tdefault:\n\t\tthrow Error( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'loss', [ 'epsilonInsensitive', 'huber', 'squaredError' ].join( '\", \"' ), opts.loss ) );\n\t}\n\n\t// Set learning rate:\n\t_getEta = getEta( opts.learningRate, opts.eta0, opts.lambda );\n\n\t/**\n\t* Update weights given new observations `y` and `x`.\n\t*\n\t* @param {NumericArray} x - feature vector\n\t* @param {number} y - response value\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t*\n\t* @example\n\t* // Update model as observations come in:\n\t* var y = 3.5;\n\t* var x = [ 2.3, 1.0, 5.0 ];\n\t* accumulator( x, y );\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tvar eta;\n\n\t\tif ( !isArray( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( !_weights ) {\n\t\t\t_weights = new WeightVector( x.length, opts.intercept );\n\t\t\t_nFeatures = opts.intercept ? _weights.nWeights - 1 : _weights.nWeights;\n\t\t}\n\t\tif ( x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array of length %u. Value: `%s`.', _nFeatures, x ) );\n\t\t}\n\n\t\t// Get current learning rate...\n\t\teta = _getEta();\n\n\t\t// Update weights depending on the chosen loss function...\n\t\t_lossfun( _weights, x, y, eta, opts.lambda, opts.epsilon );\n\t}\n\n\tsetNonEnumerableReadOnlyAccessor( accumulator, 'coefs', getCoefs );\n\tsetReadOnly( accumulator, 'predict', predict );\n\treturn accumulator;\n\n\t/**\n\t* Model coefficients / feature weights.\n\t*\n\t* @private\n\t* @name coefs\n\t* @type {Array}\n\t*\n\t* @example\n\t* // Retrieve coefficients:\n\t* var coefs = accumulator.coefs;\n\t*/\n\tfunction getCoefs() {\n\t\tvar ret;\n\t\tvar i;\n\n\t\tret = new Array( _weights.nWeights );\n\t\tfor ( i = 0; i < ret.length; i++ ) {\n\t\t\tret[ i ] = _weights._data[ i ] * _weights.scale;\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/**\n\t* Predict response for a new observation with features `x`.\n\t*\n\t* @private\n\t* @param {NumericArray} x - feature vector\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t* @returns {number} response value\n\t*\n\t* @example\n\t* // Predict new observation:\n\t* var x = [ 2.3, 5.3, 8.6 ];\n\t* var yHat = accumulator.predict( x );\n\t*/\n\tfunction predict( x ) {\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array of length %u. Value: `%s`.', _nFeatures || 0, x ) );\n\t\t}\n\t\treturn _weights.innerProduct( x );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = incrSGDRegression;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* @module @stdlib/ml-incr-sgd-regression\n*\n* @example\n* var incrSGDRegression = require( '@stdlib/ml-incr-sgd-regression' );\n*\n* var accumulator = incrSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* accumulator( x, y );\n*/\n\n// MODULES //\n\nvar main = require( './main.js' );\n\n\n// EXPORTS //\n\nmodule.exports = main;\n"],
  "mappings": "uGAAA,IAAAA,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAmCA,SAASC,EAAKC,EAAGC,EAAI,CACpB,IAAIC,EAAMF,EAAE,OACRG,EAAM,EACNC,EAEJ,IAAMA,EAAI,EAAGA,EAAIF,EAAKE,IACrBD,GAAOH,EAAGI,CAAE,EAAIH,EAAGG,CAAE,EAEtB,OAAOD,CACR,CAKAL,EAAO,QAAUC,ICjDjB,IAAAM,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cA0BA,IAAIC,EAAoB,QAAS,oCAAqC,EAClEC,EAAY,QAAS,2BAA4B,EACjDC,EAAc,QAAS,uDAAwD,EAC/EC,EAAS,QAAS,uBAAwB,EAC1CC,EAAM,QAAS,+BAAgC,EAC/CC,EAAM,IAKNC,EAAY,MAYhB,SAASC,EAASC,EAAS,CAE1B,IAAIC,EACJ,GAAK,KAAK,MAAQH,EAAY,CAE7B,IAAMG,EAAI,EAAGA,EAAI,KAAK,SAAUA,IAC/B,KAAK,MAAOA,CAAE,GAAK,KAAK,MAEzB,KAAK,MAAQ,CACd,CAIA,GAFA,KAAK,MAAQL,EAAKI,EAAQ,CAAE,EAEvBA,EAAS,EACb,KAAK,OAASA,MAEd,OAAM,IAAI,WAAYL,EAAQ,4HAA6HK,CAAO,CAAE,CAEtK,CASA,SAASE,EAAKC,EAAGC,EAAS,CAEzB,IAAIC,EACAC,EACAL,EAMJ,IAJAK,EAAQ,EACHF,IAAW,SACfA,EAAS,GAEJH,EAAI,EAAGA,EAAIE,EAAE,OAAQF,IAC1BI,EAAUF,EAAGF,CAAE,EAAIG,EACnBE,GAAS,KAAK,MAAML,CAAC,EAAII,EACzB,KAAK,MAAOJ,CAAE,EAAI,KAAK,MAAOA,CAAE,EAAMI,EAAU,KAAK,MAGjD,KAAK,YACTA,EAAU,EAAMD,EAChBE,GAAS,KAAK,MAAOL,CAAE,EAAII,EAC3B,KAAK,MAAOJ,CAAE,EAAI,KAAK,MAAOA,CAAE,EAAMI,EAAU,KAAK,OAEtD,KAAK,OAAYR,EAAKM,EAAGA,CAAE,GAAQ,KAAK,UAAc,EAAM,IAC3DP,EAAKQ,EAAQ,CAAE,EACb,EAAM,KAAK,MAAQE,CACvB,CASA,SAASC,EAAcJ,EAAI,CAE1B,IAAIK,EAAM,EACNP,EACJ,IAAMA,EAAI,EAAGA,EAAIE,EAAE,OAAQF,IAC1BO,GAAO,KAAK,MAAOP,CAAE,EAAIE,EAAGF,CAAE,EAE/B,OAAAO,GAAS,KAAK,UAAc,KAAK,MAAOP,CAAE,EAAI,EAC9CO,GAAO,KAAK,MACLA,CACR,CAcA,SAASC,EAAcC,EAAKC,EAAY,CACvC,IAAIV,EACJ,GAAK,EAAE,gBAAgBQ,GACtB,OAAO,IAAIA,EAAcC,EAAKC,CAAU,EAEzC,GAAK,CAACnB,EAAmBkB,CAAI,EAC5B,MAAM,IAAI,UAAWf,EAAQ,4EAA6Ee,CAAI,CAAE,EAEjH,GAAK,CAACjB,EAAWkB,CAAU,EAC1B,MAAM,IAAI,UAAWhB,EAAQ,oEAAqEgB,CAAU,CAAE,EAW/G,IARA,KAAK,MAAQ,EACb,KAAK,KAAO,EACZ,KAAK,UAAYA,EACjB,KAAK,SAAWD,GAAU,KAAK,UAAc,EAAI,GAEjD,KAAK,MAAQ,IAAI,MAAO,KAAK,QAAS,EAGhCT,EAAI,EAAGA,EAAI,KAAK,SAAUA,IAC/B,KAAK,MAAOA,CAAE,EAAI,CAEpB,CAUAP,EAAae,EAAa,UAAW,UAAWV,CAAQ,EAUxDL,EAAae,EAAa,UAAW,MAAOP,CAAI,EAUhDR,EAAae,EAAa,UAAW,eAAgBF,CAAa,EAKlEhB,EAAO,QAAUkB,IC9LjB,IAAAG,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,EAAM,QAAS,+BAAgC,EAK/CC,EAAqB,KAazB,SAASC,EAAYC,EAASC,EAAQC,EAAM,CAC3C,IAAIC,EACCF,EAAS,IACbE,EAAgB,EAAQD,EAAMD,EAC9BD,EAAQ,QAASH,EAAKM,EAAeL,CAAmB,CAAE,EAE5D,CAKAF,EAAO,QAAUG,ICnDjB,IAAAK,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAa,IAoBjB,SAASC,GAAwBC,EAASC,EAAGC,EAAGC,EAAKC,EAAQC,EAAU,CACtE,IAAIC,EAAIN,EAAQ,aAAcC,CAAE,EAAIC,EAGpCJ,GAAYE,EAASI,EAAQD,CAAI,EAE5BG,EAAID,EACRL,EAAQ,IAAKC,EAAG,CAACE,CAAI,EACVG,EAAI,CAACD,GAChBL,EAAQ,IAAKC,EAAG,CAACE,CAAI,CAEvB,CAKAN,EAAO,QAAUE,KC1DjB,IAAAQ,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAa,IAmBjB,SAASC,GAAkBC,EAASC,EAAGC,EAAGC,EAAKC,EAAS,CACvD,IAAIC,EAAOH,EAAIF,EAAQ,aAAcC,CAAE,EAGvCH,GAAYE,EAASI,EAAQD,CAAI,EAEjCH,EAAQ,IAAKC,EAAKE,EAAME,CAAO,CAChC,CAKAR,EAAO,QAAUE,KCrDjB,IAAAO,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAa,IAsBjB,SAASC,GAAWC,EAASC,EAAGC,EAAGC,EAAKC,EAAQC,EAAU,CACzD,IAAIC,EAAIN,EAAQ,aAAcC,CAAE,EAAIC,EAGpCJ,GAAYE,EAASI,EAAQD,CAAI,EAE5BG,EAAID,EACRL,EAAQ,IAAKC,EAAG,CAACE,CAAI,EACVG,EAAI,CAACD,EAChBL,EAAQ,IAAKC,EAAG,CAACE,CAAI,EAErBH,EAAQ,IAAKC,EAAG,CAACE,EAAMG,CAAE,CAE3B,CAKAT,EAAO,QAAUE,KC9DjB,IAAAQ,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAS,QAAS,uBAAwB,EAe9C,SAASC,GAASC,EAAMC,EAAMC,EAAS,CACtC,IAAIC,EACAC,EAIJ,OAFAD,EAAO,EAEEH,EAAO,CAChB,IAAK,QAEJI,EAAMC,EACN,MACD,IAAK,WACJD,EAAME,EACN,MACD,IAAK,UACJF,EAAMG,EACN,MACD,QACC,MAAM,IAAI,MAAOT,GAAQ,gFAAiF,eAAgB,CAAE,QAAS,WAAY,SAAU,EAAE,KAAM,MAAO,EAAGE,CAAK,CAAE,CACrL,CACA,OAAOI,EAQP,SAASC,GAAc,CACtB,IAAIG,EAAM,KAAWL,EAAO,KAC5B,OAAAA,GAAQ,EACDK,CACR,CAQA,SAASF,GAAiB,CACzB,OAAAH,GAAQ,EACDF,CACR,CAQA,SAASM,GAAgB,CACxB,IAAIC,EAAM,GAAQN,EAASC,GAC3B,OAAAA,GAAQ,EACDK,CACR,CACD,CAKAX,EAAO,QAAUE,KClGjB,IAAAU,EAAAC,EAAA,SAAAC,GAAAC,GAAA,CAAAA,GAAA,SACI,QAAW,GACX,KAAQ,IACR,UAAa,GACb,OAAU,KACV,aAAgB,QAChB,KAAQ,cACZ,ICPA,IAAAC,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAsB,QAAS,sCAAuC,EAAE,YACxEC,EAAmB,QAAS,mCAAoC,EAAE,YAClEC,GAAY,QAAS,2BAA4B,EAAE,YACnDC,GAAW,QAAS,gCAAiC,EACrDC,EAAW,QAAS,0BAA2B,EAAE,YACjDC,EAAa,QAAS,iCAAkC,EACxDC,EAAS,QAAS,uBAAwB,EA2B9C,SAASC,GAAUC,EAAMC,EAAU,CAClC,OAAMN,GAAUM,CAAQ,EAGnBJ,EAAYI,EAAS,SAAU,IACnCD,EAAK,QAAUC,EAAQ,QAClB,CAACR,EAAkBO,EAAK,OAAQ,GAC7B,IAAI,UAAWF,EAAQ,uEAAwE,UAAWE,EAAK,OAAQ,CAAE,EAG7HH,EAAYI,EAAS,MAAO,IAChCD,EAAK,KAAOC,EAAQ,KACf,CAACR,EAAkBO,EAAK,IAAK,GAC1B,IAAI,UAAWF,EAAQ,uEAAwE,OAAQE,EAAK,IAAK,CAAE,EAGvHH,EAAYI,EAAS,QAAS,IAClCD,EAAK,OAASC,EAAQ,OACjB,CAACT,GAAqBQ,EAAK,MAAO,GAC/B,IAAI,UAAWF,EAAQ,0EAA2E,SAAUE,EAAK,MAAO,CAAE,EAG9HH,EAAYI,EAAS,cAAe,IACxCD,EAAK,aAAeC,EAAQ,aACvB,CAACL,EAAUI,EAAK,YAAa,GAC1B,IAAI,UAAWF,EAAQ,8DAA+D,eAAgBE,EAAK,YAAa,CAAE,EAG9HH,EAAYI,EAAS,MAAO,IAChCD,EAAK,KAAOC,EAAQ,KACf,CAACL,EAAUI,EAAK,IAAK,GAClB,IAAI,UAAWF,EAAQ,8DAA+D,OAAQE,EAAK,IAAK,CAAE,EAG9GH,EAAYI,EAAS,WAAY,IACrCD,EAAK,UAAYC,EAAQ,UACpB,CAACP,GAAWM,EAAK,SAAU,GACxB,IAAI,UAAWF,EAAQ,+DAAgE,YAAaE,EAAK,SAAU,CAAE,EAGvH,KAtCC,IAAI,UAAWF,EAAQ,qEAAsEG,CAAQ,CAAE,CAuChH,CAKAV,EAAO,QAAUQ,KCrGjB,IAAAG,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAuBA,IAAIC,EAAU,QAAS,yBAA0B,EAC7CC,EAAS,QAAS,uBAAwB,EAC1CC,GAAO,QAAS,oBAAqB,EACrCC,GAAmC,QAAS,uDAAwD,EACpGC,GAAc,QAAS,uDAAwD,EAC/EC,GAAe,IACfC,GAAyB,IACzBC,GAAmB,IACnBC,GAAY,IACZC,GAAS,IACTC,GAAW,IACXC,GAAW,IA8Cf,SAASC,GAAmBC,EAAU,CACrC,IAAIC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EAGJ,GADAD,EAAOjB,GAAMQ,EAAS,EACjB,UAAU,OAAS,IACvBU,EAAMT,GAAUQ,EAAMN,CAAQ,EACzBO,GACJ,MAAMA,EAMR,OAHAJ,EAAW,KAGFG,EAAK,KAAO,CACrB,IAAK,qBACJJ,EAAWT,GACZ,MACA,IAAK,QACJS,EAAWP,GACZ,MACA,IAAK,eACJO,EAAWR,GACZ,MACA,QACC,MAAM,MAAON,EAAQ,gFAAiF,OAAQ,CAAE,qBAAsB,QAAS,cAAe,EAAE,KAAM,MAAO,EAAGkB,EAAK,IAAK,CAAE,CAC7L,CAGAF,EAAUR,GAAQU,EAAK,aAAcA,EAAK,KAAMA,EAAK,MAAO,EAgB5D,SAASD,EAAaG,EAAGC,EAAI,CAC5B,IAAIC,EAEJ,GAAK,CAACvB,EAASqB,CAAE,EAChB,MAAM,IAAI,UAAWpB,EAAQ,kEAAmEoB,CAAE,CAAE,EAMrG,GAJML,IACLA,EAAW,IAAIX,GAAcgB,EAAE,OAAQF,EAAK,SAAU,EACtDL,EAAaK,EAAK,UAAYH,EAAS,SAAW,EAAIA,EAAS,UAE3DK,EAAE,SAAWP,EACjB,MAAM,IAAI,UAAWb,EAAQ,+EAAgFa,EAAYO,CAAE,CAAE,EAI9HE,EAAMN,EAAQ,EAGdF,EAAUC,EAAUK,EAAGC,EAAGC,EAAKJ,EAAK,OAAQA,EAAK,OAAQ,CAC1D,CAEA,OAAAhB,GAAkCe,EAAa,QAASM,CAAS,EACjEpB,GAAac,EAAa,UAAWO,CAAQ,EACtCP,EAaP,SAASM,GAAW,CACnB,IAAIE,EACAC,EAGJ,IADAD,EAAM,IAAI,MAAOV,EAAS,QAAS,EAC7BW,EAAI,EAAGA,EAAID,EAAI,OAAQC,IAC5BD,EAAKC,CAAE,EAAIX,EAAS,MAAOW,CAAE,EAAIX,EAAS,MAE3C,OAAOU,CACR,CAgBA,SAASD,EAASJ,EAAI,CACrB,GAAK,CAACrB,EAASqB,CAAE,GAAKA,EAAE,SAAWP,EAClC,MAAM,IAAI,UAAWb,EAAQ,+EAAgFa,GAAc,EAAGO,CAAE,CAAE,EAEnI,OAAOL,EAAS,aAAcK,CAAE,CACjC,CACD,CAKAtB,EAAO,QAAUa,KClKjB,IAAIgB,GAAO,IAKX,OAAO,QAAUA",
  "names": ["require_dot", "__commonJSMin", "exports", "module", "dot", "x", "y", "len", "ret", "i", "require_weight_vector", "__commonJSMin", "exports", "module", "isPositiveInteger", "isBoolean", "setReadOnly", "format", "pow", "dot", "MIN_SCALE", "scaleTo", "factor", "i", "add", "x", "xScale", "xscaled", "inner", "innerProduct", "ret", "WeightVector", "dim", "intercept", "require_regularize", "__commonJSMin", "exports", "module", "max", "MIN_SCALING_FACTOR", "regularize", "weights", "lambda", "eta", "scalingFactor", "require_epsilon_insensitive", "__commonJSMin", "exports", "module", "regularize", "epsilonInsensitiveLoss", "weights", "x", "y", "eta", "lambda", "epsilon", "p", "require_squared_error", "__commonJSMin", "exports", "module", "regularize", "squaredErrorLoss", "weights", "x", "y", "eta", "lambda", "loss", "require_huber", "__commonJSMin", "exports", "module", "regularize", "huberLoss", "weights", "x", "y", "eta", "lambda", "epsilon", "p", "require_eta_factory", "__commonJSMin", "exports", "module", "format", "closure", "type", "eta0", "lambda", "iter", "ret", "getEtaBasic", "getEtaConstant", "getEtaPegasos", "eta", "require_defaults", "__commonJSMin", "exports", "module", "require_validate", "__commonJSMin", "exports", "module", "isNonNegativeNumber", "isPositiveNumber", "isBoolean", "isObject", "isString", "hasOwnProp", "format", "validate", "opts", "options", "require_main", "__commonJSMin", "exports", "module", "isArray", "format", "copy", "setNonEnumerableReadOnlyAccessor", "setReadOnly", "WeightVector", "epsilonInsensitiveLoss", "squaredErrorLoss", "huberLoss", "getEta", "DEFAULTS", "validate", "incrSGDRegression", "options", "_nFeatures", "_lossfun", "_weights", "_getEta", "accumulator", "opts", "err", "x", "y", "eta", "getCoefs", "predict", "ret", "i", "main"]
}
