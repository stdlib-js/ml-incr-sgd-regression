{"version":3,"file":"index.mjs","sources":["../lib/weight_vector.js","../lib/dot.js","../lib/regularize.js","../lib/loss/epsilon_insensitive.js","../lib/loss/squared_error.js","../lib/loss/huber.js","../lib/eta_factory.js","../lib/validate.js","../lib/main.js"],"sourcesContent":["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* The weight vector implementation was inspired by the [sofia-ml]{@link https://code.google.com/archive/p/sofia-ml/} library.\n*/\n\n// MODULES //\n\nimport isPositiveInteger from '@stdlib/assert-is-positive-integer';\nimport isBoolean from '@stdlib/assert-is-boolean';\nimport setReadOnly from '@stdlib/utils-define-nonenumerable-read-only-property';\nimport format from '@stdlib/error-tools-fmtprodmsg';\nimport pow from '@stdlib/math-base-special-pow';\nimport dot from './dot.js';\n\n\n// VARIABLES //\n\nvar MIN_SCALE = 1.0e-11;\n\n\n// FUNCTIONS //\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @private\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nfunction scaleTo( factor ) {\n\t/* eslint-disable no-invalid-this */\n\tvar i;\n\tif ( this.scale < MIN_SCALE ) {\n\t\t// Scale vector to one:\n\t\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\t\tthis._data[ i ] *= this.scale;\n\t\t}\n\t\tthis.scale = 1.0;\n\t}\n\n\tthis.norm *= pow( factor, 2 );\n\n\tif ( factor > 0.0 ) {\n\t\tthis.scale *= factor;\n\t} else {\n\t\tthrow new RangeError( format( 'unexpected error. Scaling weight vector by nonpositive value, likely due to too large value of eta * lambda. Value: `%f`.', factor ) );\n\t}\n}\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @private\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nfunction add( x, xScale ) {\n\t/* eslint-disable no-invalid-this */\n\tvar xscaled;\n\tvar inner;\n\tvar i;\n\n\tinner = 0.0;\n\tif ( xScale === void 0 ) {\n\t\txScale = 1.0;\n\t}\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\txscaled = x[ i ] * xScale;\n\t\tinner += this._data[i] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this.intercept ) {\n\t\txscaled = 1.0 * xScale;\n\t\tinner += this._data[ i ] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\tthis.norm += ( ( dot( x, x ) + ( ( this.intercept ) ? 1.0 : 0.0 ) ) *\n\t\tpow( xScale, 2 ) ) +\n\t\t( 2.0 * this.scale * inner );\n}\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @private\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nfunction innerProduct( x ) {\n\t/* eslint-disable no-invalid-this */\n\tvar ret = 0;\n\tvar i;\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\tret += this._data[ i ] * x[ i ];\n\t}\n\tret += ( this.intercept ) ? this._data[ i ] : 0.0;\n\tret *= this.scale;\n\treturn ret;\n}\n\n\n// MAIN //\n\n/**\n* Creates a WeightVector.\n*\n* @constructor\n* @param {PositiveInteger} dim - number of feature weights (excluding bias/intercept term)\n* @param {boolean} intercept - boolean indicating whether a bias/intercept weight should be implicitly assumed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a boolean\n*/\nfunction WeightVector( dim, intercept ) {\n\tvar i;\n\tif ( !(this instanceof WeightVector) ) {\n\t\treturn new WeightVector( dim, intercept );\n\t}\n\tif ( !isPositiveInteger( dim ) ) {\n\t\tthrow new TypeError( format( '0LT4o', dim ) );\n\t}\n\tif ( !isBoolean( intercept ) ) {\n\t\tthrow new TypeError( format( '0LT6G', intercept ) );\n\t}\n\n\tthis.scale = 1.0;\n\tthis.norm = 0.0;\n\tthis.intercept = intercept;\n\tthis.nWeights = dim + ( ( this.intercept ) ? 1 : 0 );\n\n\tthis._data = new Array( this.nWeights );\n\n\t// Initialize weights to zero:\n\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\tthis._data[ i ] = 0.0;\n\t}\n}\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @memberof WeightVector.prototype\n* @function scaleTo\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nsetReadOnly( WeightVector.prototype, 'scaleTo', scaleTo );\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @memberof WeightVector.prototype\n* @function add\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nsetReadOnly( WeightVector.prototype, 'add', add );\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @memberof WeightVector.prototype\n* @function innerProduct\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nsetReadOnly( WeightVector.prototype, 'innerProduct', innerProduct );\n\n\n// EXPORTS //\n\nexport default WeightVector;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Calculates the dot product of two vectors.\n*\n* @private\n* @param {NumericArray} x - first vector\n* @param {NumericArray} y - second vector\n* @returns {number} dot product\n*\n* @example\n* var x = [ 1.0, 2.0, 3.0 ];\n* var y = [ 1.0, 2.0, 2.0 ];\n*\n* var ret = dot( x, y );\n* // returns 11.0\n*/\nfunction dot( x, y ) {\n\tvar len = x.length;\n\tvar ret = 0;\n\tvar i;\n\n\tfor ( i = 0; i < len; i++ ) {\n\t\tret += x[ i ] * y[ i ];\n\t}\n\treturn ret;\n}\n\n\n// EXPORTS //\n\nexport default dot;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport max from '@stdlib/math-base-special-max';\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1e-7;\n\n\n// MAIN //\n\n/**\n* L2 regularization of feature weights.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} eta - current learning rate\n*/\nfunction regularize( weights, lambda, eta ) {\n\tvar scalingFactor;\n\tif ( lambda > 0.0 ) {\n\t\tscalingFactor = 1.0 - ( eta * lambda );\n\t\tweights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default regularize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport regularize from './../regularize.js';\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the epsilon-insensitive loss.\n*\n* ## Notes\n*\n* The penalty of the epsilon-insensitive loss is the absolute value of the dot product of the weights and `x` minus `y` whenever the absolute error exceeds epsilon, and zero otherwise.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction epsilonInsensitiveLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default epsilonInsensitiveLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport regularize from './../regularize.js';\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the squared error loss.\n*\n* ## Notes\n*\n* The squared error loss is defined as the squared difference of the observed and fitted value.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction squaredErrorLoss( weights, x, y, eta, lambda ) {\n\tvar loss = y - weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tweights.add( x, ( eta * loss ) );\n}\n\n\n// EXPORTS //\n\nexport default squaredErrorLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport regularize from './../regularize.js';\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the [Huber loss][1] function.\n*\n* ## Notes\n*\n* The Huber loss uses squared-error loss for observations with error smaller than epsilon in magnitude and linear loss above that in order to decrease the influence of outliers on the model fit.\n*\n* [1]: https://en.wikipedia.org/wiki/Huber_loss\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction huberLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t} else {\n\t\tweights.add( x, -eta * p );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default huberLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport format from '@stdlib/error-tools-fmtprodmsg';\n\n\n// MAIN //\n\n/**\n* Returns a function to retrieve the current learning rate.\n*\n* @private\n* @param {string} type - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {PositiveNumber} eta0 - constant learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @throws {Error} first argument must be `basic`, `constant` or `pegasos`\n* @returns {Function} getEta function\n*/\nfunction closure( type, eta0, lambda ) {\n\tvar iter;\n\tvar ret;\n\n\titer = 1;\n\n\tswitch ( type ) {\n\tcase 'basic':\n\t\t// Default case: 'basic'\n\t\tret = getEtaBasic;\n\t\tbreak;\n\tcase 'constant':\n\t\tret = getEtaConstant;\n\t\tbreak;\n\tcase 'pegasos':\n\t\tret = getEtaPegasos;\n\t\tbreak;\n\tdefault:\n\t\tthrow new Error( format( '0LT3t', 'learningRate', [ 'basic', 'constant', 'pegasos' ].join( '\", \"' ), type ) );\n\t}\n\treturn ret;\n\n\t/**\n\t* Returns the basic learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaBasic() {\n\t\tvar eta = 1000.0 / ( iter + 1000.0 );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n\n\t/**\n\t* Returns the constant learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaConstant() {\n\t\titer += 1;\n\t\treturn eta0;\n\t}\n\n\t/**\n\t* Returns the Pegasos learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaPegasos() {\n\t\tvar eta = 1.0 / ( lambda * iter );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default closure;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isNonNegativeNumber } from '@stdlib/assert-is-nonnegative-number';\nimport { isPrimitive as isPositiveNumber } from '@stdlib/assert-is-positive-number';\nimport { isPrimitive as isBoolean } from '@stdlib/assert-is-boolean';\nimport isObject from '@stdlib/assert-is-plain-object';\nimport { isPrimitive as isString } from '@stdlib/assert-is-string';\nimport hasOwnProp from '@stdlib/assert-has-own-property';\nimport format from '@stdlib/error-tools-fmtprodmsg';\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.epsilon] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0] - constant learning rate\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {string} [options.learningRate] - the learning rate to use\n* @param {string} [options.loss] -  the loss function to use\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( '0LT2h', options ) );\n\t}\n\tif ( hasOwnProp( options, 'epsilon' ) ) {\n\t\topts.epsilon = options.epsilon;\n\t\tif ( !isPositiveNumber( opts.epsilon ) ) {\n\t\t\treturn new TypeError( format( '0LT4Q', 'epsilon', opts.epsilon ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'eta0' ) ) {\n\t\topts.eta0 = options.eta0;\n\t\tif ( !isPositiveNumber( opts.eta0 ) ) {\n\t\t\treturn new TypeError( format( '0LT4Q', 'eta0', opts.eta0 ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( '0LT4x', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\topts.learningRate = options.learningRate;\n\t\tif ( !isString( opts.learningRate ) ) {\n\t\t\treturn new TypeError( format( '0LT2i', 'learningRate', opts.learningRate ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isString( opts.loss ) ) {\n\t\t\treturn new TypeError( format( '0LT2i', 'loss', opts.loss ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( '0LT30', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nexport default validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable */ // TODO: fix me\n'use strict';\n\n// MODULES //\n\nimport isArray from '@stdlib/assert-is-array';\nimport format from '@stdlib/error-tools-fmtprodmsg';\nimport copy from '@stdlib/utils-copy';\nimport setNonEnumerableReadOnlyAccessor from '@stdlib/utils-define-nonenumerable-read-only-accessor';\nimport setReadOnly from '@stdlib/utils-define-nonenumerable-read-only-property';\nimport WeightVector from './weight_vector.js';\nimport epsilonInsensitiveLoss from './loss/epsilon_insensitive.js';\nimport squaredErrorLoss from './loss/squared_error.js';\nimport huberLoss from './loss/huber.js';\nimport getEta from './eta_factory.js';\nimport DEFAULTS from './defaults.json';\nimport validate from './validate.js';\n\n\n// MAIN //\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* The sub-gradient of the loss function is estimated for each datum and the regression model is updated incrementally, with a decreasing learning rate and regularization of the feature weights based on L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3–30. doi:10.1007/s10107-010-0420-4\n*\n* @param {Object} [options] - options object\n* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate\n* @param {NonNegativeNumber} [options.lambda=1e-3] - regularization parameter\n* @param {string} [options.learningRate='basic'] - string denoting the learning rate to use. Can be `constant`, `pegasos`, or `basic`\n* @param {string} [options.loss='squaredError'] - string denoting the loss function to use. Can be `squaredError`, `epsilonInsensitive`, or `huber`\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Object} regression model\n*\n* @example\n* import incrSGDRegression from '@stdlib/streams-ml-incr-sgd-regression';\n*\n* var accumulator = incrSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* // Update model as observations come in:\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* accumulator( x, y );\n*\n* // Predict new observation:\n* var yHat = accumulator.predict( x );\n*\n* // Retrieve coefficients:\n* var coefs = accumulator.coefs;\n*/\nfunction incrSGDRegression( options ) {\n\tvar _nFeatures;\n\tvar _lossfun;\n\tvar _weights;\n\tvar _getEta;\n\tvar accumulator;\n\tvar opts;\n\tvar err;\n\n\topts = copy( DEFAULTS );\n\tif ( arguments.length > 0 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\t_weights = null;\n\n\t// Set loss function:\n\tswitch ( opts.loss ) {\n\tcase 'epsilonInsensitive':\n\t\t_lossfun = epsilonInsensitiveLoss;\n\tbreak;\n\tcase 'huber':\n\t\t_lossfun = huberLoss;\n\tbreak;\n\tcase 'squaredError':\n\t\t_lossfun = squaredErrorLoss;\n\tbreak;\n\tdefault:\n\t\tthrow Error( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'loss', [ 'epsilonInsensitive', 'huber', 'squaredError' ].join( '\", \"' ), opts.loss ) );\n\t}\n\n\t// Set learning rate:\n\t_getEta = getEta( opts.learningRate, opts.eta0, opts.lambda );\n\n\t/**\n\t* Update weights given new observations `y` and `x`.\n\t*\n\t* @param {NumericArray} x - feature vector\n\t* @param {number} y - response value\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t*\n\t* @example\n\t* // Update model as observations come in:\n\t* var y = 3.5;\n\t* var x = [ 2.3, 1.0, 5.0 ];\n\t* accumulator( x, y );\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tvar eta;\n\n\t\tif ( !isArray( x ) ) {\n\t\t\tthrow new TypeError( format( '0LTBj', x ) );\n\t\t}\n\t\tif ( !_weights ) {\n\t\t\t_weights = new WeightVector( x.length, opts.intercept );\n\t\t\t_nFeatures = opts.intercept ? _weights.nWeights - 1 : _weights.nWeights;\n\t\t}\n\t\tif ( x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array of length %u. Value: `%s`.', _nFeatures, x ) );\n\t\t}\n\n\t\t// Get current learning rate...\n\t\teta = _getEta();\n\n\t\t// Update weights depending on the chosen loss function...\n\t\t_lossfun( _weights, x, y, eta, opts.lambda, opts.epsilon );\n\t}\n\n\tsetNonEnumerableReadOnlyAccessor( accumulator, 'coefs', getCoefs );\n\tsetReadOnly( accumulator, 'predict', predict );\n\treturn accumulator;\n\n\t/**\n\t* Model coefficients / feature weights.\n\t*\n\t* @private\n\t* @name coefs\n\t* @type {Array}\n\t*\n\t* @example\n\t* // Retrieve coefficients:\n\t* var coefs = accumulator.coefs;\n\t*/\n\tfunction getCoefs() {\n\t\tvar ret;\n\t\tvar i;\n\n\t\tret = new Array( _weights.nWeights );\n\t\tfor ( i = 0; i < ret.length; i++ ) {\n\t\t\tret[ i ] = _weights._data[ i ] * _weights.scale;\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/**\n\t* Predict response for a new observation with features `x`.\n\t*\n\t* @private\n\t* @param {NumericArray} x - feature vector\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t* @returns {number} response value\n\t*\n\t* @example\n\t* // Predict new observation:\n\t* var x = [ 2.3, 5.3, 8.6 ];\n\t* var yHat = accumulator.predict( x );\n\t*/\n\tfunction predict( x ) {\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array of length %u. Value: `%s`.', _nFeatures || 0, x ) );\n\t\t}\n\t\treturn _weights.innerProduct( x );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrSGDRegression;\n"],"names":["WeightVector","dim","intercept","i","this","isPositiveInteger","TypeError","format","isBoolean","scale","norm","nWeights","_data","Array","setReadOnly","prototype","factor","pow","RangeError","x","xScale","xscaled","inner","length","y","len","ret","dot","regularize","weights","lambda","eta","scalingFactor","scaleTo","max","epsilonInsensitiveLoss","epsilon","p","innerProduct","add","squaredErrorLoss","loss","huberLoss","closure","type","eta0","iter","Error","join","validate","opts","options","isObject","hasOwnProp","isPositiveNumber","isNonNegativeNumber","learningRate","isString","incrSGDRegression","_nFeatures","_lossfun","_weights","_getEta","accumulator","err","copy","DEFAULTS","arguments","isArray","getEta","setNonEnumerableReadOnlyAccessor","getCoefs","predict"],"mappings":";;u0CAoIA,SAASA,EAAcC,EAAKC,GAC3B,IAAIC,EACJ,KAAOC,gBAAgBJ,GACtB,OAAO,IAAIA,EAAcC,EAAKC,GAE/B,IAAMG,EAAmBJ,GACxB,MAAM,IAAIK,UAAWC,EAAQ,QAASN,IAEvC,IAAMO,EAAWN,GAChB,MAAM,IAAII,UAAWC,EAAQ,QAASL,IAWvC,IARAE,KAAKK,MAAQ,EACbL,KAAKM,KAAO,EACZN,KAAKF,UAAYA,EACjBE,KAAKO,SAAWV,GAAUG,KAAmB,UAAA,EAAI,GAEjDA,KAAKQ,MAAQ,IAAIC,MAAOT,KAAKO,UAGvBR,EAAI,EAAGA,EAAIC,KAAKO,SAAUR,IAC/BC,KAAKQ,MAAOT,GAAM,CAEpB,CAUAW,EAAad,EAAae,UAAW,WArHrC,SAAkBC,GAEjB,IAAIb,EACJ,GAAKC,KAAKK,MAfK,MAee,CAE7B,IAAMN,EAAI,EAAGA,EAAIC,KAAKO,SAAUR,IAC/BC,KAAKQ,MAAOT,IAAOC,KAAKK,MAEzBL,KAAKK,MAAQ,CACb,CAID,GAFAL,KAAKM,MAAQO,EAAKD,EAAQ,KAErBA,EAAS,GAGb,MAAM,IAAIE,WAAYX,EAAQ,4HAA6HS,IAF3JZ,KAAKK,OAASO,CAIhB,IA6GAF,EAAad,EAAae,UAAW,OApGrC,SAAcI,EAAGC,GAEhB,IAAIC,EACAC,EACAnB,EAMJ,IAJAmB,EAAQ,OACQ,IAAXF,IACJA,EAAS,GAEJjB,EAAI,EAAGA,EAAIgB,EAAEI,OAAQpB,IAC1BkB,EAAUF,EAAGhB,GAAMiB,EACnBE,GAASlB,KAAKQ,MAAMT,GAAKkB,EACzBjB,KAAKQ,MAAOT,GAAMC,KAAKQ,MAAOT,GAAQkB,EAAUjB,KAAKK,MAGjDL,KAAKF,YACTmB,EAAU,EAAMD,EAChBE,GAASlB,KAAKQ,MAAOT,GAAMkB,EAC3BjB,KAAKQ,MAAOT,GAAMC,KAAKQ,MAAOT,GAAQkB,EAAUjB,KAAKK,OAEtDL,KAAKM,OC7DN,SAAcS,EAAGK,GAChB,IAEIrB,EAFAsB,EAAMN,EAAEI,OACRG,EAAM,EAGV,IAAMvB,EAAI,EAAGA,EAAIsB,EAAKtB,IACrBuB,GAAOP,EAAGhB,GAAMqB,EAAGrB,GAEpB,OAAOuB,CACR,CDoDkBC,CAAKR,EAAGA,IAAUf,KAAmB,UAAA,EAAM,IAC3Da,EAAKG,EAAQ,GACX,EAAMhB,KAAKK,MAAQa,CACvB,IAsFAR,EAAad,EAAae,UAAW,gBA7ErC,SAAuBI,GAEtB,IACIhB,EADAuB,EAAM,EAEV,IAAMvB,EAAI,EAAGA,EAAIgB,EAAEI,OAAQpB,IAC1BuB,GAAOtB,KAAKQ,MAAOT,GAAMgB,EAAGhB,GAI7B,OAFAuB,GAAStB,KAAc,UAAKA,KAAKQ,MAAOT,GAAM,EAC9CuB,GAAOtB,KAAKK,KAEb,IE9EA,SAASmB,EAAYC,EAASC,EAAQC,GACrC,IAAIC,EACCF,EAAS,IACbE,EAAgB,EAAQD,EAAMD,EAC9BD,EAAQI,QAASC,EAAKF,EAjBC,OAmBzB,CCJA,SAASG,EAAwBN,EAASV,EAAGK,EAAGO,EAAKD,EAAQM,GAC5D,IAAIC,EAAIR,EAAQS,aAAcnB,GAAMK,EAGpCI,EAAYC,EAASC,EAAQC,GAExBM,EAAID,EACRP,EAAQU,IAAKpB,GAAIY,GACNM,GAAKD,GAChBP,EAAQU,IAAKpB,GAAIY,EAEnB,CCZA,SAASS,EAAkBX,EAASV,EAAGK,EAAGO,EAAKD,GAC9C,IAAIW,EAAOjB,EAAIK,EAAQS,aAAcnB,GAGrCS,EAAYC,EAASC,EAAQC,GAE7BF,EAAQU,IAAKpB,EAAKY,EAAMU,EACzB,CCJA,SAASC,EAAWb,EAASV,EAAGK,EAAGO,EAAKD,EAAQM,GAC/C,IAAIC,EAAIR,EAAQS,aAAcnB,GAAMK,EAGpCI,EAAYC,EAASC,EAAQC,GAExBM,EAAID,EACRP,EAAQU,IAAKpB,GAAIY,GACNM,GAAKD,EAChBP,EAAQU,IAAKpB,GAAIY,GAEjBF,EAAQU,IAAKpB,GAAIY,EAAMM,EAEzB,CCpBA,SAASM,EAASC,EAAMC,EAAMf,GAC7B,IAAIgB,EACApB,EAIJ,OAFAoB,EAAO,EAEEF,GACT,IAAK,QAEJlB,EAmBD,WACC,IAAIK,EAAM,KAAWe,EAAO,KAE5B,OADAA,GAAQ,EACDf,CACP,EAtBA,MACD,IAAK,WACJL,EA4BD,WAEC,OADAoB,GAAQ,EACDD,CACP,EA9BA,MACD,IAAK,UACJnB,EAoCD,WACC,IAAIK,EAAM,GAAQD,EAASgB,GAE3B,OADAA,GAAQ,EACDf,CACP,EAvCA,MACD,QACC,MAAM,IAAIgB,MAAOxC,EAAQ,QAAS,eAAgB,CAAE,QAAS,WAAY,WAAYyC,KAAM,QAAUJ,IAEtG,OAAOlB,CAoCR,+FCtCA,SAASuB,EAAUC,EAAMC,GACxB,OAAMC,EAAUD,GAGXE,EAAYF,EAAS,aACzBD,EAAKd,QAAUe,EAAQf,SACjBkB,EAAkBJ,EAAKd,UACrB,IAAI9B,UAAWC,EAAQ,QAAS,UAAW2C,EAAKd,UAGpDiB,EAAYF,EAAS,UACzBD,EAAKL,KAAOM,EAAQN,MACdS,EAAkBJ,EAAKL,OACrB,IAAIvC,UAAWC,EAAQ,QAAS,OAAQ2C,EAAKL,OAGjDQ,EAAYF,EAAS,YACzBD,EAAKpB,OAASqB,EAAQrB,QAChByB,EAAqBL,EAAKpB,SACxB,IAAIxB,UAAWC,EAAQ,QAAS,SAAU2C,EAAKpB,SAGnDuB,EAAYF,EAAS,kBACzBD,EAAKM,aAAeL,EAAQK,cACtBC,EAAUP,EAAKM,eACb,IAAIlD,UAAWC,EAAQ,QAAS,eAAgB2C,EAAKM,eAGzDH,EAAYF,EAAS,UACzBD,EAAKT,KAAOU,EAAQV,MACdgB,EAAUP,EAAKT,OACb,IAAInC,UAAWC,EAAQ,QAAS,OAAQ2C,EAAKT,OAGjDY,EAAYF,EAAS,eACzBD,EAAKhD,UAAYiD,EAAQjD,WACnBM,EAAW0C,EAAKhD,YACd,IAAII,UAAWC,EAAQ,QAAS,YAAa2C,EAAKhD,YAGpD,KAtCC,IAAII,UAAWC,EAAQ,QAAS4C,GAuCzC,CChBA,SAASO,EAAmBP,GAC3B,IAAIQ,EACAC,EACAC,EACAC,EACAC,EACAb,EACAc,EAGJ,GADAd,EAAOe,EAAMC,GACRC,UAAU5C,OAAS,IACvByC,EAAMf,EAAUC,EAAMC,IAErB,MAAMa,EAMR,OAHAH,EAAW,KAGFX,EAAKT,MACd,IAAK,qBACJmB,EAAWzB,EACZ,MACA,IAAK,QACJyB,EAAWlB,EACZ,MACA,IAAK,eACJkB,EAAWpB,EACZ,MACA,QACC,MAAMO,MAAOxC,EAAQ,gFAAiF,OAAQ,CAAE,qBAAsB,QAAS,gBAAiByC,KAAM,QAAUE,EAAKT,OAoBtL,SAASsB,EAAa5C,EAAGK,GACxB,IAAIO,EAEJ,IAAMqC,EAASjD,GACd,MAAM,IAAIb,UAAWC,EAAQ,QAASY,IAMvC,GAJM0C,IACLA,EAAW,IAAI7D,EAAcmB,EAAEI,OAAQ2B,EAAKhD,WAC5CyD,EAAaT,EAAKhD,UAAY2D,EAASlD,SAAW,EAAIkD,EAASlD,UAE3DQ,EAAEI,SAAWoC,EACjB,MAAM,IAAIrD,UAAWC,EAAQ,+EAAgFoD,EAAYxC,IAI1HY,EAAM+B,IAGNF,EAAUC,EAAU1C,EAAGK,EAAGO,EAAKmB,EAAKpB,OAAQoB,EAAKd,QACjD,CAID,OAvCA0B,EAAUO,EAAQnB,EAAKM,aAAcN,EAAKL,KAAMK,EAAKpB,QAqCrDwC,EAAkCP,EAAa,QAASQ,GACxDzD,EAAaiD,EAAa,UAAWS,GAC9BT,EAaP,SAASQ,IACR,IAAI7C,EACAvB,EAGJ,IADAuB,EAAM,IAAIb,MAAOgD,EAASlD,UACpBR,EAAI,EAAGA,EAAIuB,EAAIH,OAAQpB,IAC5BuB,EAAKvB,GAAM0D,EAASjD,MAAOT,GAAM0D,EAASpD,MAE3C,OAAOiB,CACP,CAgBD,SAAS8C,EAASrD,GACjB,IAAMiD,EAASjD,IAAOA,EAAEI,SAAWoC,EAClC,MAAM,IAAIrD,UAAWC,EAAQ,+EAAgFoD,GAAc,EAAGxC,IAE/H,OAAO0C,EAASvB,aAAcnB,EAC9B,CACF"}