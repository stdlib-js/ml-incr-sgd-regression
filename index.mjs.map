{"version":3,"file":"index.mjs","sources":["../lib/dot.js","../lib/weight_vector.js","../lib/regularize.js","../lib/loss/epsilon_insensitive.js","../lib/loss/squared_error.js","../lib/loss/huber.js","../lib/eta_factory.js","../lib/validate.js","../lib/main.js","../lib/index.js"],"sourcesContent":["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Calculates the dot product of two vectors.\n*\n* @private\n* @param {NumericArray} x - first vector\n* @param {NumericArray} y - second vector\n* @returns {number} dot product\n*\n* @example\n* var x = [ 1.0, 2.0, 3.0 ];\n* var y = [ 1.0, 2.0, 2.0 ];\n*\n* var ret = dot( x, y );\n* // returns 11.0\n*/\nfunction dot( x, y ) {\n\tvar len = x.length;\n\tvar ret = 0;\n\tvar i;\n\n\tfor ( i = 0; i < len; i++ ) {\n\t\tret += x[ i ] * y[ i ];\n\t}\n\treturn ret;\n}\n\n\n// EXPORTS //\n\nmodule.exports = dot;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* The weight vector implementation was inspired by the [sofia-ml]{@link https://code.google.com/archive/p/sofia-ml/} library.\n*/\n\n// MODULES //\n\nvar isPositiveInteger = require( '@stdlib/assert-is-positive-integer' );\nvar isBoolean = require( '@stdlib/assert-is-boolean' );\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar format = require( '@stdlib/error-tools-fmtprodmsg' );\nvar pow = require( '@stdlib/math-base-special-pow' );\nvar dot = require( './dot.js' );\n\n\n// VARIABLES //\n\nvar MIN_SCALE = 1.0e-11;\n\n\n// FUNCTIONS //\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @private\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nfunction scaleTo( factor ) {\n\t/* eslint-disable no-invalid-this */\n\tvar i;\n\tif ( this.scale < MIN_SCALE ) {\n\t\t// Scale vector to one:\n\t\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\t\tthis._data[ i ] *= this.scale;\n\t\t}\n\t\tthis.scale = 1.0;\n\t}\n\n\tthis.norm *= pow( factor, 2 );\n\n\tif ( factor > 0.0 ) {\n\t\tthis.scale *= factor;\n\t} else {\n\t\tthrow new RangeError( format( '0LT5H', factor ) );\n\t}\n}\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @private\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nfunction add( x, xScale ) {\n\t/* eslint-disable no-invalid-this */\n\tvar xscaled;\n\tvar inner;\n\tvar i;\n\n\tinner = 0.0;\n\tif ( xScale === void 0 ) {\n\t\txScale = 1.0;\n\t}\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\txscaled = x[ i ] * xScale;\n\t\tinner += this._data[i] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this.intercept ) {\n\t\txscaled = 1.0 * xScale;\n\t\tinner += this._data[ i ] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\tthis.norm += ( ( dot( x, x ) + ( ( this.intercept ) ? 1.0 : 0.0 ) ) *\n\t\tpow( xScale, 2 ) ) +\n\t\t( 2.0 * this.scale * inner );\n}\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @private\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nfunction innerProduct( x ) {\n\t/* eslint-disable no-invalid-this */\n\tvar ret = 0;\n\tvar i;\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\tret += this._data[ i ] * x[ i ];\n\t}\n\tret += ( this.intercept ) ? this._data[ i ] : 0.0;\n\tret *= this.scale;\n\treturn ret;\n}\n\n\n// MAIN //\n\n/**\n* Creates a WeightVector.\n*\n* @constructor\n* @param {PositiveInteger} dim - number of feature weights (excluding bias/intercept term)\n* @param {boolean} intercept - boolean indicating whether a bias/intercept weight should be implicitly assumed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a boolean\n*/\nfunction WeightVector( dim, intercept ) {\n\tvar i;\n\tif ( !(this instanceof WeightVector) ) {\n\t\treturn new WeightVector( dim, intercept );\n\t}\n\tif ( !isPositiveInteger( dim ) ) {\n\t\tthrow new TypeError( format( '0LT5I', dim ) );\n\t}\n\tif ( !isBoolean( intercept ) ) {\n\t\tthrow new TypeError( format( '0LT5J', intercept ) );\n\t}\n\n\tthis.scale = 1.0;\n\tthis.norm = 0.0;\n\tthis.intercept = intercept;\n\tthis.nWeights = dim + ( ( this.intercept ) ? 1 : 0 );\n\n\tthis._data = new Array( this.nWeights );\n\n\t// Initialize weights to zero:\n\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\tthis._data[ i ] = 0.0;\n\t}\n}\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @memberof WeightVector.prototype\n* @function scaleTo\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nsetReadOnly( WeightVector.prototype, 'scaleTo', scaleTo );\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @memberof WeightVector.prototype\n* @function add\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nsetReadOnly( WeightVector.prototype, 'add', add );\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @memberof WeightVector.prototype\n* @function innerProduct\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nsetReadOnly( WeightVector.prototype, 'innerProduct', innerProduct );\n\n\n// EXPORTS //\n\nmodule.exports = WeightVector;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar max = require( '@stdlib/math-base-special-max' );\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1e-7;\n\n\n// MAIN //\n\n/**\n* L2 regularization of feature weights.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} eta - current learning rate\n*/\nfunction regularize( weights, lambda, eta ) {\n\tvar scalingFactor;\n\tif ( lambda > 0.0 ) {\n\t\tscalingFactor = 1.0 - ( eta * lambda );\n\t\tweights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = regularize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar regularize = require( './../regularize.js' );\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the epsilon-insensitive loss.\n*\n* ## Notes\n*\n* The penalty of the epsilon-insensitive loss is the absolute value of the dot product of the weights and `x` minus `y` whenever the absolute error exceeds epsilon, and zero otherwise.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction epsilonInsensitiveLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = epsilonInsensitiveLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar regularize = require( './../regularize.js' );\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the squared error loss.\n*\n* ## Notes\n*\n* The squared error loss is defined as the squared difference of the observed and fitted value.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction squaredErrorLoss( weights, x, y, eta, lambda ) {\n\tvar loss = y - weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tweights.add( x, ( eta * loss ) );\n}\n\n\n// EXPORTS //\n\nmodule.exports = squaredErrorLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar regularize = require( './../regularize.js' );\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the [Huber loss][1] function.\n*\n* ## Notes\n*\n* The Huber loss uses squared-error loss for observations with error smaller than epsilon in magnitude and linear loss above that in order to decrease the influence of outliers on the model fit.\n*\n* [1]: https://en.wikipedia.org/wiki/Huber_loss\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction huberLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t} else {\n\t\tweights.add( x, -eta * p );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = huberLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar format = require( '@stdlib/error-tools-fmtprodmsg' );\n\n\n// MAIN //\n\n/**\n* Returns a function to retrieve the current learning rate.\n*\n* @private\n* @param {string} type - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {PositiveNumber} eta0 - constant learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @throws {Error} first argument must be `basic`, `constant` or `pegasos`\n* @returns {Function} getEta function\n*/\nfunction closure( type, eta0, lambda ) {\n\tvar iter;\n\tvar ret;\n\n\titer = 1;\n\n\tswitch ( type ) {\n\tcase 'basic':\n\t\t// Default case: 'basic'\n\t\tret = getEtaBasic;\n\t\tbreak;\n\tcase 'constant':\n\t\tret = getEtaConstant;\n\t\tbreak;\n\tcase 'pegasos':\n\t\tret = getEtaPegasos;\n\t\tbreak;\n\tdefault:\n\t\tthrow new Error( format( '0LT5E', type ) );\n\t}\n\treturn ret;\n\n\t/**\n\t* Returns the basic learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaBasic() {\n\t\tvar eta = 1000.0 / ( iter + 1000.0 );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n\n\t/**\n\t* Returns the constant learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaConstant() {\n\t\titer += 1;\n\t\treturn eta0;\n\t}\n\n\t/**\n\t* Returns the Pegasos learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaPegasos() {\n\t\tvar eta = 1.0 / ( lambda * iter );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = closure;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar isNonNegativeNumber = require( '@stdlib/assert-is-nonnegative-number' ).isPrimitive;\nvar isPositiveNumber = require( '@stdlib/assert-is-positive-number' ).isPrimitive;\nvar isBoolean = require( '@stdlib/assert-is-boolean' ).isPrimitive;\nvar isObject = require( '@stdlib/assert-is-plain-object' );\nvar isString = require( '@stdlib/assert-is-string' ).isPrimitive;\nvar hasOwnProp = require( '@stdlib/assert-has-own-property' );\nvar format = require( '@stdlib/error-tools-fmtprodmsg' );\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.epsilon] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0] - constant learning rate\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {string} [options.learningRate] - the learning rate to use\n* @param {string} [options.loss] -  the loss function to use\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( '0LT2h', options ) );\n\t}\n\tif ( hasOwnProp( options, 'epsilon' ) ) {\n\t\topts.epsilon = options.epsilon;\n\t\tif ( !isPositiveNumber( opts.epsilon ) ) {\n\t\t\treturn new TypeError( format( '0LT4Q', 'epsilon', opts.epsilon ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'eta0' ) ) {\n\t\topts.eta0 = options.eta0;\n\t\tif ( !isPositiveNumber( opts.eta0 ) ) {\n\t\t\treturn new TypeError( format( '0LT4Q', 'eta0', opts.eta0 ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( '0LT4x', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\topts.learningRate = options.learningRate;\n\t\tif ( !isString( opts.learningRate ) ) {\n\t\t\treturn new TypeError( format( '0LT2i', 'learningRate', opts.learningRate ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isString( opts.loss ) ) {\n\t\t\treturn new TypeError( format( '0LT2i', 'loss', opts.loss ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( '0LT30', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nmodule.exports = validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable */ // TODO: fix me\n'use strict';\n\n// MODULES //\n\nvar isArray = require( '@stdlib/assert-is-array' );\nvar format = require( '@stdlib/error-tools-fmtprodmsg' );\nvar copy = require( '@stdlib/utils-copy' );\nvar setNonEnumerableReadOnlyAccessor = require( '@stdlib/utils-define-nonenumerable-read-only-accessor' );\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar WeightVector = require( './weight_vector.js' );\nvar epsilonInsensitiveLoss = require( './loss/epsilon_insensitive.js' );\nvar squaredErrorLoss = require( './loss/squared_error.js' );\nvar huberLoss = require( './loss/huber.js' );\nvar getEta = require( './eta_factory.js' );\nvar DEFAULTS = require( './defaults.json' );\nvar validate = require( './validate.js' );\n\n\n// MAIN //\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* The sub-gradient of the loss function is estimated for each datum and the regression model is updated incrementally, with a decreasing learning rate and regularization of the feature weights based on L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3â€“30. doi:10.1007/s10107-010-0420-4\n*\n* @param {Object} [options] - options object\n* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate\n* @param {NonNegativeNumber} [options.lambda=1e-3] - regularization parameter\n* @param {string} [options.learningRate='basic'] - string denoting the learning rate to use. Can be `constant`, `pegasos`, or `basic`\n* @param {string} [options.loss='squaredError'] - string denoting the loss function to use. Can be `squaredError`, `epsilonInsensitive`, or `huber`\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Object} regression model\n*\n* @example\n* var incrSGDRegression = require( '@stdlib/streams-ml-incr-sgd-regression' );\n*\n* var accumulator = incrSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* // Update model as observations come in:\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* accumulator( x, y );\n*\n* // Predict new observation:\n* var yHat = accumulator.predict( x );\n*\n* // Retrieve coefficients:\n* var coefs = accumulator.coefs;\n*/\nfunction incrSGDRegression( options ) {\n\tvar _nFeatures;\n\tvar _lossfun;\n\tvar _weights;\n\tvar _getEta;\n\tvar accumulator;\n\tvar opts;\n\tvar err;\n\n\topts = copy( DEFAULTS );\n\tif ( arguments.length > 0 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\t_weights = null;\n\n\t// Set loss function:\n\tswitch ( opts.loss ) {\n\tcase 'epsilonInsensitive':\n\t\t_lossfun = epsilonInsensitiveLoss;\n\tbreak;\n\tcase 'huber':\n\t\t_lossfun = huberLoss;\n\tbreak;\n\tcase 'squaredError':\n\t\t_lossfun = squaredErrorLoss;\n\tbreak;\n\tdefault:\n\t\tthrow Error( format( 'invalid input value. `loss` option must be either `epsilonInsensitive`, `huber` or `squaredError`. Value: `%s`.', opts.loss ) );\n\t}\n\n\t// Set learning rate:\n\t_getEta = getEta( opts.learningRate, opts.eta0, opts.lambda );\n\n\t/**\n\t* Update weights given new observations `y` and `x`.\n\t*\n\t* @param {NumericArray} x - feature vector\n\t* @param {number} y - response value\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t*\n\t* @example\n\t* // Update model as observations come in:\n\t* var y = 3.5;\n\t* var x = [ 2.3, 1.0, 5.0 ];\n\t* accumulator( x, y );\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tvar eta;\n\n\t\tif ( !isArray( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid input value. First argument `x` must be an array. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( !_weights ) {\n\t\t\t_weights = new WeightVector( x.length, opts.intercept );\n\t\t\t_nFeatures = opts.intercept ? _weights.nWeights - 1 : _weights.nWeights;\n\t\t}\n\t\tif ( x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid input value. First argument `x` must be an array of length `%u`. Value: `%s`', _nFeatures, x ) );\n\t\t}\n\n\t\t// Get current learning rate...\n\t\teta = _getEta();\n\n\t\t// Update weights depending on the chosen loss function...\n\t\t_lossfun( _weights, x, y, eta, opts.lambda, opts.epsilon );\n\t}\n\n\tsetNonEnumerableReadOnlyAccessor( accumulator, 'coefs', getCoefs );\n\tsetReadOnly( accumulator, 'predict', predict );\n\treturn accumulator;\n\n\t/**\n\t* Model coefficients / feature weights.\n\t*\n\t* @private\n\t* @name coefs\n\t* @type {Array}\n\t*\n\t* @example\n\t* // Retrieve coefficients:\n\t* var coefs = accumulator.coefs;\n\t*/\n\tfunction getCoefs() {\n\t\tvar ret;\n\t\tvar i;\n\n\t\tret = new Array( _weights.nWeights );\n\t\tfor ( i = 0; i < ret.length; i++ ) {\n\t\t\tret[ i ] = _weights._data[ i ] * _weights.scale;\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/**\n\t* Predict response for a new observation with features `x`.\n\t*\n\t* @private\n\t* @param {NumericArray} x - feature vector\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t* @returns {number} response value\n\t*\n\t* @example\n\t* // Predict new observation:\n\t* var x = [ 2.3, 5.3, 8.6 ];\n\t* var yHat = accumulator.predict( x );\n\t*/\n\tfunction predict( x ) {\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( '0LT5G', _nFeatures || 0, x ) );\n\t\t}\n\t\treturn _weights.innerProduct( x );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = incrSGDRegression;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* @module @stdlib/ml-incr-sgd-regression\n*\n* @example\n* var incrSGDRegression = require( '@stdlib/ml-incr-sgd-regression' );\n*\n* var accumulator = incrSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* accumulator( x, y );\n*/\n\n// MODULES //\n\nvar incrSGDRegression = require( './main.js' );\n\n\n// EXPORTS //\n\nmodule.exports = incrSGDRegression;\n"],"names":["isPositiveInteger","require$$0","isBoolean","require$$1","setReadOnly","require$$2","format","require$$3","pow","require$$4","dot","x","y","i","len","length","ret","WeightVector","dim","intercept","this","TypeError","scale","norm","nWeights","_data","Array","prototype","factor","RangeError","xScale","xscaled","inner","weight_vector","max","regularize_1","weights","lambda","eta","scalingFactor","scaleTo","regularize","epsilon_insensitive","epsilon","p","innerProduct","add","squared_error","loss","huber","eta_factory","type","eta0","iter","Error","isNonNegativeNumber","isPrimitive","isPositiveNumber","isObject","isString","hasOwnProp","require$$5","require$$6","isArray","copy","setNonEnumerableReadOnlyAccessor","epsilonInsensitiveLoss","squaredErrorLoss","require$$7","huberLoss","require$$8","getEta","require$$9","DEFAULTS","require$$10","validate","opts","options","learningRate","lib","_nFeatures","_lossfun","_weights","_getEta","accumulator","err","arguments","getCoefs","predict"],"mappings":";;wwCAiDA,ICvBIA,EAAoBC,EACpBC,EAAYC,EACZC,EAAcC,EACdC,EAASC,EACTC,EAAMC,EACNC,EDIJ,SAAcC,EAAGC,GAChB,IAEIC,EAFAC,EAAMH,EAAEI,OACRC,EAAM,EAGV,IAAMH,EAAI,EAAGA,EAAIC,EAAKD,IACrBG,GAAOL,EAAGE,GAAMD,EAAGC,GAEpB,OAAOG,GCyFR,SAASC,EAAcC,EAAKC,GAC3B,IAAIN,EACJ,KAAOO,gBAAgBH,GACtB,OAAO,IAAIA,EAAcC,EAAKC,GAE/B,IAAMnB,EAAmBkB,GACxB,MAAM,IAAIG,UAAWf,EAAQ,QAASY,IAEvC,IAAMhB,EAAWiB,GAChB,MAAM,IAAIE,UAAWf,EAAQ,QAASa,IAWvC,IARAC,KAAKE,MAAQ,EACbF,KAAKG,KAAO,EACZH,KAAKD,UAAYA,EACjBC,KAAKI,SAAWN,GAAUE,KAAmB,UAAA,EAAI,GAEjDA,KAAKK,MAAQ,IAAIC,MAAON,KAAKI,UAGvBX,EAAI,EAAGA,EAAIO,KAAKI,SAAUX,IAC/BO,KAAKK,MAAOZ,GAAM,EAYpBT,EAAaa,EAAaU,UAAW,WArHrC,SAAkBC,GAEjB,IAAIf,EACJ,GAAKO,KAAKE,MAfK,MAee,CAE7B,IAAMT,EAAI,EAAGA,EAAIO,KAAKI,SAAUX,IAC/BO,KAAKK,MAAOZ,IAAOO,KAAKE,MAEzBF,KAAKE,MAAQ,EAKd,GAFAF,KAAKG,MAAQf,EAAKoB,EAAQ,KAErBA,EAAS,GAGb,MAAM,IAAIC,WAAYvB,EAAQ,QAASsB,IAFvCR,KAAKE,OAASM,KAiHhBxB,EAAaa,EAAaU,UAAW,OApGrC,SAAchB,EAAGmB,GAEhB,IAAIC,EACAC,EACAnB,EAMJ,IAJAmB,EAAQ,OACQ,IAAXF,IACJA,EAAS,GAEJjB,EAAI,EAAGA,EAAIF,EAAEI,OAAQF,IAC1BkB,EAAUpB,EAAGE,GAAMiB,EACnBE,GAASZ,KAAKK,MAAMZ,GAAKkB,EACzBX,KAAKK,MAAOZ,GAAMO,KAAKK,MAAOZ,GAAQkB,EAAUX,KAAKE,MAGjDF,KAAKD,YACTY,EAAU,EAAMD,EAChBE,GAASZ,KAAKK,MAAOZ,GAAMkB,EAC3BX,KAAKK,MAAOZ,GAAMO,KAAKK,MAAOZ,GAAQkB,EAAUX,KAAKE,OAEtDF,KAAKG,OAAYb,EAAKC,EAAGA,IAAUS,KAAmB,UAAA,EAAM,IAC3DZ,EAAKsB,EAAQ,GACX,EAAMV,KAAKE,MAAQU,KAuFvB5B,EAAaa,EAAaU,UAAW,gBA7ErC,SAAuBhB,GAEtB,IACIE,EADAG,EAAM,EAEV,IAAMH,EAAI,EAAGA,EAAIF,EAAEI,OAAQF,IAC1BG,GAAOI,KAAKK,MAAOZ,GAAMF,EAAGE,GAI7B,OAFAG,GAASI,KAAc,UAAKA,KAAKK,MAAOZ,GAAM,EAC9CG,GAAOI,KAAKE,SA0Eb,IAAAW,EAAiBhB,ECxKbiB,EAAMjC,EA6BV,IAAAkC,EAXA,SAAqBC,EAASC,EAAQC,GACrC,IAAIC,EACCF,EAAS,IACbE,EAAgB,EAAQD,EAAMD,EAC9BD,EAAQI,QAASN,EAAKK,EAjBC,SCLrBE,EAAaxC,EAoCjB,IAAAyC,EAhBA,SAAiCN,EAASzB,EAAGC,EAAG0B,EAAKD,EAAQM,GAC5D,IAAIC,EAAIR,EAAQS,aAAclC,GAAMC,EAGpC6B,EAAYL,EAASC,EAAQC,GAExBM,EAAID,EACRP,EAAQU,IAAKnC,GAAI2B,GACNM,GAAKD,GAChBP,EAAQU,IAAKnC,GAAI2B,IC7BfG,EAAaxC,EA+BjB,IAAA8C,EAZA,SAA2BX,EAASzB,EAAGC,EAAG0B,EAAKD,GAC9C,IAAIW,EAAOpC,EAAIwB,EAAQS,aAAclC,GAGrC8B,EAAYL,EAASC,EAAQC,GAE7BF,EAAQU,IAAKnC,EAAK2B,EAAMU,ICzBrBP,EAAaxC,EAwCjB,IAAAgD,EAlBA,SAAoBb,EAASzB,EAAGC,EAAG0B,EAAKD,EAAQM,GAC/C,IAAIC,EAAIR,EAAQS,aAAclC,GAAMC,EAGpC6B,EAAYL,EAASC,EAAQC,GAExBM,EAAID,EACRP,EAAQU,IAAKnC,GAAI2B,GACNM,GAAKD,EAChBP,EAAQU,IAAKnC,GAAI2B,GAEjBF,EAAQU,IAAKnC,GAAI2B,EAAMM,ICjCrBtC,EAASL,EA4Eb,IAAAiD,EA7DA,SAAkBC,EAAMC,EAAMf,GAC7B,IAAIgB,EACArC,EAIJ,OAFAqC,EAAO,EAEEF,GACT,IAAK,QAEJnC,EAmBD,WACC,IAAIsB,EAAM,KAAWe,EAAO,KAE5B,OADAA,GAAQ,EACDf,GArBP,MACD,IAAK,WACJtB,EA4BD,WAEC,OADAqC,GAAQ,EACDD,GA7BP,MACD,IAAK,UACJpC,EAoCD,WACC,IAAIsB,EAAM,GAAQD,EAASgB,GAE3B,OADAA,GAAQ,EACDf,GAtCP,MACD,QACC,MAAM,IAAIgB,MAAOhD,EAAQ,QAAS6C,IAEnC,OAAOnC,6FCnCJuC,EAAsBtD,EAAkDuD,YACxEC,EAAmBtD,EAA+CqD,YAClEtD,EAAYG,EAAuCmD,YACnDE,EAAWnD,EACXoD,EAAWlD,EAAsC+C,YACjDI,EAAaC,EACbvD,EAASwD,EAyEb,IC9EIC,EAAU9D,EACVK,EAASH,EACT6D,EAAO3D,EACP4D,EAAmC1D,EACnCH,EAAcK,EACdQ,EAAe4C,EACfK,EAAyBJ,EACzBK,EAAmBC,EACnBC,EAAYC,EACZC,EAASC,EACTC,EAAWC,EACXC,EDqBJ,SAAmBC,EAAMC,GACxB,OAAMnB,EAAUmB,GAGXjB,EAAYiB,EAAS,aACzBD,EAAKjC,QAAUkC,EAAQlC,SACjBc,EAAkBmB,EAAKjC,UACrB,IAAItB,UAAWf,EAAQ,QAAS,UAAWsE,EAAKjC,UAGpDiB,EAAYiB,EAAS,UACzBD,EAAKxB,KAAOyB,EAAQzB,MACdK,EAAkBmB,EAAKxB,OACrB,IAAI/B,UAAWf,EAAQ,QAAS,OAAQsE,EAAKxB,OAGjDQ,EAAYiB,EAAS,YACzBD,EAAKvC,OAASwC,EAAQxC,QAChBkB,EAAqBqB,EAAKvC,SACxB,IAAIhB,UAAWf,EAAQ,QAAS,SAAUsE,EAAKvC,SAGnDuB,EAAYiB,EAAS,kBACzBD,EAAKE,aAAeD,EAAQC,cACtBnB,EAAUiB,EAAKE,eACb,IAAIzD,UAAWf,EAAQ,QAAS,eAAgBsE,EAAKE,eAGzDlB,EAAYiB,EAAS,UACzBD,EAAK5B,KAAO6B,EAAQ7B,MACdW,EAAUiB,EAAK5B,OACb,IAAI3B,UAAWf,EAAQ,QAAS,OAAQsE,EAAK5B,OAGjDY,EAAYiB,EAAS,eACzBD,EAAKzD,UAAY0D,EAAQ1D,WACnBjB,EAAW0E,EAAKzD,YACd,IAAIE,UAAWf,EAAQ,QAAS,YAAasE,EAAKzD,YAGpD,KAtCC,IAAIE,UAAWf,EAAQ,QAASuE,KCiJzC,IC7JAE,EDmCA,SAA4BF,GAC3B,IAAIG,EACAC,EACAC,EACAC,EACAC,EACAR,EACAS,EAGJ,GADAT,EAAOZ,EAAMS,GACRa,UAAUvE,OAAS,IACvBsE,EAAMV,EAAUC,EAAMC,IAErB,MAAMQ,EAMR,OAHAH,EAAW,KAGFN,EAAK5B,MACd,IAAK,qBACJiC,EAAWf,EACZ,MACA,IAAK,QACJe,EAAWZ,EACZ,MACA,IAAK,eACJY,EAAWd,EACZ,MACA,QACC,MAAMb,MAAOhD,EAAQ,kHAAmHsE,EAAK5B,OAoB9I,SAASoC,EAAazE,EAAGC,GACxB,IAAI0B,EAEJ,IAAMyB,EAASpD,GACd,MAAM,IAAIU,UAAWf,EAAQ,yEAA0EK,IAMxG,GAJMuE,IACLA,EAAW,IAAIjE,EAAcN,EAAEI,OAAQ6D,EAAKzD,WAC5C6D,EAAaJ,EAAKzD,UAAY+D,EAAS1D,SAAW,EAAI0D,EAAS1D,UAE3Db,EAAEI,SAAWiE,EACjB,MAAM,IAAI3D,UAAWf,EAAQ,uFAAwF0E,EAAYrE,IAIlI2B,EAAM6C,IAGNF,EAAUC,EAAUvE,EAAGC,EAAG0B,EAAKsC,EAAKvC,OAAQuC,EAAKjC,SAKlD,OAvCAwC,EAAUZ,EAAQK,EAAKE,aAAcF,EAAKxB,KAAMwB,EAAKvC,QAqCrD4B,EAAkCmB,EAAa,QAASG,GACxDnF,EAAagF,EAAa,UAAWI,GAC9BJ,EAaP,SAASG,IACR,IAAIvE,EACAH,EAGJ,IADAG,EAAM,IAAIU,MAAOwD,EAAS1D,UACpBX,EAAI,EAAGA,EAAIG,EAAID,OAAQF,IAC5BG,EAAKH,GAAMqE,EAASzD,MAAOZ,GAAMqE,EAAS5D,MAE3C,OAAON,EAiBR,SAASwE,EAAS7E,GACjB,IAAMoD,EAASpD,IAAOA,EAAEI,SAAWiE,EAClC,MAAM,IAAI3D,UAAWf,EAAQ,QAAS0E,GAAc,EAAGrE,IAExD,OAAOuE,EAASrC,aAAclC"}